{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FacialKeyPoint.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shailesh008/FacialKeypointRecognition/blob/master/FacialKeyPoint.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "D5eqmG5_h37r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import cv2                     # OpenCV library for computer vision\n",
        "from PIL import Image\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oLa4WQUfjFdq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "image = cv2.imread('images/test_image_1.jpg')\n",
        "\n",
        "# Convert the image to RGB colorspace\n",
        "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Plot our image using subplots to specify a size and title\n",
        "fig = plt.figure(figsize = (8,8))\n",
        "ax1 = fig.add_subplot(111)\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "\n",
        "ax1.set_title('Original Image')\n",
        "ax1.imshow(image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a7KtzPwJlAjP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "28150989-b5b3-4d40-a935-349bda555e5a"
      },
      "cell_type": "code",
      "source": [
        "from utils import *\n",
        "\n",
        "# Load training set\n",
        "X_train, y_train = load_data()\n",
        "print(\"X_train.shape == {}\".format(X_train.shape))\n",
        "print(\"y_train.shape == {}; y_train.min == {:.3f}; y_train.max == {:.3f}\".format(\n",
        "    y_train.shape, y_train.min(), y_train.max()))\n",
        "\n",
        "# Load testing set\n",
        "X_test, _ = load_data(test=True)\n",
        "print(\"X_test.shape == {}\".format(X_test.shape))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-0ab4dfde86b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_train.shape == {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m print(\"y_train.shape == {}; y_train.min == {:.3f}; y_train.max == {:.3f}\".format(\n\u001b[1;32m      4\u001b[0m     y_train.shape, y_train.min(), y_train.max()))\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_data' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "i2UnZT5GlFaN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "for i in range(9):\n",
        "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
        "    plot_data(X_train[i], y_train[i], ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-znI0e6YlMbD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Import deep learning resources from Keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout\n",
        "from keras.layers import Flatten, Dense\n",
        "\n",
        "\n",
        "## TODO: Specify a CNN architecture\n",
        "# Your model should accept 96x96 pixel graysale images in\n",
        "# It should have a fully-connected output layer with 30 values (2 for each facial keypoint)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Define your architecture.\n",
        "model.add(Conv2D(filters=32, kernel_size=3, activation='relu', \n",
        "                        input_shape=(96, 96, 1)))\n",
        "\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3,3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(212, activation='relu'))\n",
        "model.add(Dense(30))\n",
        "\n",
        "# Summarize the model\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CrZoMKQVlSVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.optimizers import SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
        "from keras.callbacks import ModelCheckpoint \n",
        "\n",
        "## TODO: Compile the model\n",
        "model.compile(Adamax(), loss='mean_squared_error', metrics=['accuracy'])\n",
        "\n",
        "# Save the model weights\n",
        "checkpointer = ModelCheckpoint(filepath='saved_models/weights_facial_keypoints.h5', \n",
        "                               verbose=1, save_best_only=True)\n",
        "\n",
        "## TODO: Train the model\n",
        "history = model.fit(X_train, y_train, \n",
        "          validation_split=0.2,\n",
        "          epochs=50, batch_size=20, callbacks=[checkpointer], verbose=1)\n",
        "\n",
        "## TODO: Save the model as model.h5\n",
        "model.save('my_model.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XZKSdbcilW4I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## TODO: Visualize the training and validation loss of your neural network\n",
        "model.load_weights('./my_model.h5')\n",
        "\n",
        "# Plotting\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tBqnj4KSlazZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "y_test = model.predict(X_test)\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "for i in range(9):\n",
        "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
        "    plot_data(X_test[i], y_test[i], ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qa4uzjq6lhGr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load in color image for face detection\n",
        "image = cv2.imread('images/obamas4.jpg')\n",
        "\n",
        "# Convert the image to RGB colorspace\n",
        "image_copy = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# plot the image\n",
        "fig = plt.figure(figsize = (9,9))\n",
        "ax1 = fig.add_subplot(111)\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "ax1.set_title('image copy')\n",
        "ax1.imshow(image_copy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t7G6BVcLlwUo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### TODO: Use the face detection code we saw in Section 1 with your trained conv-net \n",
        "## TODO : Paint the predicted keypoints on the test image\n",
        "# Extract the pre-trained face detector from an xml file\n",
        "face_cascade = cv2.CascadeClassifier('detector_architectures/haarcascade_frontalface_default.xml')\n",
        "\n",
        "# Convert the RGB  image to grayscale\n",
        "gray = cv2.cvtColor(image_copy, cv2.COLOR_RGB2GRAY)\n",
        "\n",
        "# Detect the faces in image\n",
        "faces = face_cascade.detectMultiScale(gray, 1.25, 6)\n",
        "\n",
        "# Make a copy of the orginal image to draw face detections on\n",
        "image_with_detections = np.copy(image_copy)\n",
        "\n",
        "for(x,y,w,h) in faces:\n",
        "     # Add a red bounding box to the detections image\n",
        "    cv2.rectangle(image_with_detections, (x,y), (x+w,y+h), (255,0,0), 3)\n",
        "\n",
        "# plot our image\n",
        "fig = plt.figure(figsize = (9,9))\n",
        "ax1 = fig.add_subplot(111)\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "ax1.set_title('image_with_detections')\n",
        "ax1.imshow(image_with_detections)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bbIPMZKSlzo9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cropped_faces = []\n",
        "for(x,y,w,h) in faces:\n",
        "    # crop the section where the face is located\n",
        "    face = gray[y:y+h,x:x+w]\n",
        "    # resize and normalize\n",
        "    face = cv2.resize(face, (96, 96)) / 255\n",
        "    # Need to expand because CNN expects conv2d_1_input to have 4 dimensions\n",
        "    face = np.expand_dims(face, axis=2)\n",
        "    cropped_faces.append(face)\n",
        "\n",
        "# load model\n",
        "model = load_model('./my_model.h5')\n",
        "X_faces = np.asarray(cropped_faces)\n",
        "y_faces = model.predict(X_faces)\n",
        "\n",
        "fig = plt.figure(figsize=(20,20))\n",
        "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
        "for i in range(len(faces)):\n",
        "    ax = fig.add_subplot(3, 3, i + 1, xticks=[], yticks=[])\n",
        "    plot_data(X_faces[i], y_faces[i], ax)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MljfMUO6l083",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot the Facial Keypoints on the Original image\n",
        "\n",
        "# plot the image\n",
        "fig = plt.figure(figsize = (9,9))\n",
        "ax1 = fig.add_subplot(111)\n",
        "ax1.set_xticks([])\n",
        "ax1.set_yticks([])\n",
        "ax1.set_title('Original Image')\n",
        "\n",
        "for i in range(len(faces)):\n",
        "    x, y, w, h = faces[i]\n",
        "    # rescaling back to original color image\n",
        "    x_keypoints = y_faces[i][0::2] * 48 + 48\n",
        "    x_scaled = (x_keypoints * w / 96) + x\n",
        "    y_keypoints = y_faces[i][1::2] * 48 + 48\n",
        "    y_scaled = (y_keypoints * h / 96) + y\n",
        "\n",
        "    ax1.scatter(x_scaled, y_scaled, marker='.', c='b', s=20)\n",
        "\n",
        "ax1.imshow(image_copy)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}